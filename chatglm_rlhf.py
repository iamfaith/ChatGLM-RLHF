import os
os.chdir(os.path.dirname(os.path.abspath(__file__)))
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"
import inspect
import sys
print(sys.path)
import torch
from itertools import chain
import json
from pathlib import Path
from tqdm import tqdm
from pprint import pprint
from transformers import AutoTokenizer, AutoModel
# from chatglm_local.modeling_chatglm import ChatGLMForConditionalGeneration

import sys, os
# sys.path.append("/home/faith/chatglm2-6b")
now_dir = os.path.dirname(os.path.abspath(__file__))
project_dir = os.path.dirname(now_dir)
sys.path.append(project_dir)

from chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration

from models_rlhf import Critic, Reward, RewardBySimilarity
from utils_chatglm import generate_inputs
import random
from collections import defaultdict

# set device
action_device = "cuda:1"
RM_device = "cpu" #"cuda:0"
RM_device = "cuda:0"
critic_device = "cuda:0" # "cpu" 

reward_model = RewardBySimilarity(device=RM_device)

tokenizer = AutoTokenizer.from_pretrained("/home/faith/chatglm2-6b", trust_remote_code=True)
if "cuda" in action_device:
    model = ChatGLMForConditionalGeneration.from_pretrained("/home/faith/chatglm2-6b", trust_remote_code=True)
    model = model.half().cuda(action_device) # half for gpu only
elif "cpu" == action_device:
    model = ChatGLMForConditionalGeneration.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).bfloat16()

critic = Critic(device=critic_device, m=model.transformer)

from peft import get_peft_config, get_peft_model, PrefixTuningConfig, TaskType, PeftType, AdaLoraConfig
# Define prefix tuning config
# https://huggingface.co/docs/peft/task_guides/seq2seq-prefix-tuning
# peft_config = PrefixTuningConfig(
#     peft_type=PeftType.PREFIX_TUNING,
#     task_type=TaskType.SEQ_2_SEQ_LM,
#     inference_mode=False,
#     # prefix_length=4,
#     # prefix_dropout=0.1,
#     num_virtual_tokens=20
# )

# # Wrap model with prefix tuning
# model = get_peft_model(model, peft_config)


#è®­ç»ƒæ—¶èŠ‚çº¦GPUå ç”¨
model.config.use_cache=False
model.supports_gradient_checkpointing = True  #
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

peft_config = AdaLoraConfig(
    task_type=TaskType.CAUSAL_LM, inference_mode=False,
    r=8,
    lora_alpha=32, lora_dropout=0.1,
    target_modules=["query", "value"]
)

model = get_peft_model(model, peft_config)

model.is_parallelizable = True
model.model_parallel = True
model.print_trainable_parameters()
model = model.half()


#################################### old
# # åªæ›´æ–°embedding
# model.requires_grad_(False)
# # model.transformer.word_embeddings.requires_grad_(True)
# model.get_input_embeddings().requires_grad_(True)
#################################### old



# model.lm_head.requires_grad_(True)
# model.lm_head.weightå’Œmodel.transformer.word_embeddings.weightæ˜¯å…±äº«å‚æ•°ï¼Œä¸¤è€…æ˜¯è½¬ç½®å…³ç³»


decay_up_matrix_T = None
def get_decay_up_matrix_T(dtype=torch.float, device="cpu", max_length = 2048, gamma=0.99, tau=0.95):
    global decay_up_matrix_T
    if decay_up_matrix_T is None:
        # ç”Ÿæˆè¡°å‡çŸ©é˜µ
        decay = gamma*tau
        decay_row = torch.ones(max_length, dtype=dtype, device=device)*decay
        decay_row[0] = 1
        decay_row_cross_time = decay_row.cumprod(dim=-1)
        assert decay_row_cross_time.sign().min() == 0
        decay_up_matrix = torch.zeros((max_length, max_length), dtype=dtype, device=device)
        for i in range(max_length):
            decay_row = decay_row_cross_time.roll(i)
            decay_row[:i] = 0 # ç¡®ä¿çœ‹ä¸è§å‰é¢çš„
            decay_up_matrix[i] = decay_row
        decay_up_matrix_T = decay_up_matrix.T# å…ˆè¿›è¡Œè½¬ç½®ï¼Œå› ä¸ºåé¢éœ€è¦ç”¨åˆ°çŸ©é˜µä¹˜æ³•
    return decay_up_matrix_T

def gae_vectorize(values, rewards, masks=None):
    """
        values:è¡¨ç¤ºå„ä¸ªæ—¶é—´æ­¥çŠ¶æ€çš„çŠ¶æ€å€¼ã€‚shape:batch_size,sequence_length
        rewards:è¡¨ç¤ºå„ä¸ªæ—¶é—´æ­¥åšå‡ºçš„åŠ¨ä½œçš„å¥–åŠ±ï¼Œå¯¹äºgptå½“å‰åŠ¨ä½œä¹Ÿæ˜¯åŠ¨ä½œå¯¹åº”çš„ä¸‹ä¸€çŠ¶æ€ã€‚æ‰€ä»¥shapeå’Œvaluesä¸€æ ·
                # æ³¨æ„è¿™é‡Œçš„rewardsè¡¨ç¤ºå½“å‰åŠ¨ä½œçŠ¶æ€çš„reward
        masks:ç”±äºæ˜¯è¦å¯¹ç”Ÿæˆçš„actionsåšgaeï¼Œä¹Ÿå°±æ˜¯æ³›åŒ–ä¼˜åŠ¿ä¼°è®¡ï¼Œ
                # æ‰€ä»¥ç±»ä¼¼ä»¥å¾€çš„maskåªéœ€è¦å¯¹paddingè¿›è¡Œmaskï¼Œ
                # å› ä¸ºpaddingçš„deltaä¼šè¢«æ”¾å…¥åŠ æƒè®¡ç®—ï¼Œè€Œactionå‰é¢çš„deltaï¼Œ
                # ç”±äºç”Ÿæˆçš„è¡°å‡çŸ©é˜µå°±æ˜¯ä¸Šä¸‰è§’çš„ï¼Œè‡ªç„¶å°±çœ‹ä¸åˆ°å‰é¢çš„ã€‚
                # 0è¡¨ç¤ºmaskï¼Œ 1è¡¨ç¤ºéœ€è¦çš„ã€‚
    """
    action_rewards = rewards.roll(-1) # å½“å‰çŠ¶æ€çš„åŠ¨ä½œçš„å¥–åŠ±æ˜¯ä¸‹ä¸€ä¸ªçŠ¶æ€å‡ºç°æ—¶ç»™å‡ºçš„ï¼Œè€Œå¥–åŠ±æ˜¯åŸºäºçŠ¶æ€è®¡ç®—çš„ï¼Œæ‰€ä»¥éœ€è¦shiftä¸€ä¸ªæ—¶é—´æ­¥å›å»
    # ä¸ºäº†å­¦åˆ°æœ€åè¾“å‡ºçš„<eop>,æ‰€ä»¥ç»™æœ€åçš„çŠ¶æ€èµ‹äºˆä¸€ä¸ªrewardsè¯•è¯•
    action_rewards = (action_rewards+rewards)/2 # å°†å¥–åŠ±åˆ†é…åˆ°æœ€åä¸¤æ­¥

    values_estimator_1_order = action_rewards + values.roll(-1) # è¿™é‡Œè¦æ³¨æ„rollæ˜¯å¾ªç¯çš„ï¼Œæ‰€ä»¥æœ€åä¸€ä½çš„å€¼å¯èƒ½ä¸èƒ½ç”¨
    deltas = values_estimator_1_order - values  #å¿…é¡»è¦action+ä¸‹ä¸€ä¸ªæ—¶åˆ»çš„å€¼å‡½æ•°å‡å»å½“å‰å€¼å‡½æ•°ï¼Œè¿™æ˜¯è¡¨ç¤ºå½“å‰actionçš„ä¼˜åŠ¿
    # get weight matrix
    decay_up_matrix_T = get_decay_up_matrix_T(dtype=deltas.dtype, device= deltas.device)
    # è®¡ç®—gae
    max_goal_length = deltas.shape[-1]
    sub_decay_up_matrix_T = decay_up_matrix_T[:max_goal_length, :max_goal_length]
    if masks is not None:
        deltas = deltas * masks
    gae = deltas.matmul(sub_decay_up_matrix_T.to(deltas.device))
    assert gae.shape == deltas.shape
    return gae

def get_log_prob(generated_outputs, input_ids, gen_method = "greedy_search"):
    # beam_search generate ç»™å‡ºæ¥çš„scoreså°±æ˜¯log_probäº†ï¼Œæ‰€ä»¥ç›´æ¥gatherè·å–å³å¯
    gen_sequences = generated_outputs.sequences[:, input_ids.shape[-1]:] 
    # let's stack the logits generated at each step to a tensor
    # è¦å°å¿ƒgreedy search æ‹¿åˆ°çš„æ˜¯scoreï¼Œéœ€è¦å†log_softmax
    # è€Œbeam_search æ‹¿åˆ°çš„å·²ç»æ˜¯log_softmaxäº†
    scores = torch.stack(generated_outputs.scores, dim=1)
    # if scores.max() >0 :
    #     gen_method = "greedy_search"
    if gen_method == "beam_search":
        log_prob_stacked = scores
    else:
        log_prob_stacked = torch.stack(generated_outputs.scores, dim=1).log_softmax(dim=-1)
    # now we need to collect the log_prob of the generated token # we need to add a dummy dim in the end to make gather work 
    log_prob = torch.gather(log_prob_stacked, 2, gen_sequences[:, :, None]).squeeze(-1)
    return log_prob

def get_log_probs_with_input_ids(states, gen_max_len):
    input_ids = states
    model_inputs = model.prepare_inputs_for_generation(input_ids)
    output = model(**model_inputs)  #å°†å·²ç»ç”Ÿæˆçš„åºåˆ—æ”¾è¿›å»è®¡ç®—ï¼Œå†æ¬¡è®¡ç®—å¾—åˆ°ç›®æ ‡actionä¹Ÿå°±æ˜¯åç»­å­—ç¬¦çš„æ¦‚ç‡æˆ–è€…log_probå€¼
    ###################################old
    # logits = output.logits[:, -(gen_max_len+1):-1].log_softmax(dim=-1) # æ¯”å…ˆsoftmaxå†logå¥½,å¤æ‚åº¦å‡å°ï¼Œå¹¶ä¸”è§£å†³äº›nané—®é¢˜
    # new_log_probs = logits.gather(dim=-1, index=input_ids[:, -gen_max_len:].unsqueeze(-1)).squeeze(-1)
    ###################################old
    logits = output.logits.log_softmax(dim=-1)
    new_log_probs = logits.gather(dim=-1, index=input_ids[:, -gen_max_len:].unsqueeze(1)).squeeze(-1)

    return new_log_probs



# è¿™æ®µä»£ç æ˜¯ç”¨PyTorchæ¡†æ¶ç¼–å†™çš„ï¼Œå®ƒçš„ç›®çš„æ˜¯ç”Ÿæˆä¸€äº›æ–°çš„æ–‡æœ¬åºåˆ—ï¼Œå¯èƒ½æ˜¯ç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆæˆ–è€…æœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚ä»£ç çš„ä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š

# - é¦–å…ˆï¼Œå®ƒä½¿ç”¨model.prepare_inputs_for_generationå‡½æ•°ï¼Œæ ¹æ®å·²ç»ç”Ÿæˆçš„éƒ¨åˆ†åºåˆ—ï¼ˆinput_idsï¼‰æ¥å‡†å¤‡æ¨¡å‹çš„è¾“å…¥ã€‚è¿™ä¸ªå‡½æ•°ä¼šæ ¹æ®æ¨¡å‹çš„ç±»å‹å’Œå‚æ•°ï¼Œå¯¹è¾“å…¥åºåˆ—è¿›è¡Œä¸€äº›å¿…è¦çš„å¤„ç†ï¼Œæ¯”å¦‚æ·»åŠ ç‰¹æ®Šçš„æ ‡è®°ç¬¦å·ã€è°ƒæ•´ç»´åº¦ç­‰ã€‚
# - ç„¶åï¼Œå®ƒä½¿ç”¨modelå‡½æ•°ï¼Œå°†æ¨¡å‹çš„è¾“å…¥ä¼ å…¥æ¨¡å‹ä¸­ï¼Œå¾—åˆ°æ¨¡å‹çš„è¾“å‡ºã€‚æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«äº†ä¸åŒå±‚æ¬¡çš„ä¿¡æ¯ï¼Œæ¯”å¦‚æ³¨æ„åŠ›æƒé‡ã€éšè—çŠ¶æ€ç­‰ã€‚æˆ‘ä»¬æœ€å…³å¿ƒçš„æ˜¯logitsï¼Œå®ƒæ˜¯ä¸€ä¸ªå¼ é‡ï¼ˆtensorï¼‰ï¼Œè¡¨ç¤ºäº†æ¯ä¸ªä½ç½®ä¸Šæ¯ä¸ªå¯èƒ½çš„å•è¯ï¼ˆæˆ–è€…å­—ç¬¦ï¼‰çš„å¾—åˆ†ï¼ˆscoreï¼‰ã€‚å¾—åˆ†è¶Šé«˜ï¼Œè¡¨ç¤ºè¯¥å•è¯ï¼ˆæˆ–è€…å­—ç¬¦ï¼‰å‡ºç°åœ¨è¯¥ä½ç½®ä¸Šçš„å¯èƒ½æ€§è¶Šå¤§ã€‚
# - æ¥ç€ï¼Œå®ƒä½¿ç”¨log_softmaxå‡½æ•°ï¼Œå¯¹logitsè¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œä½¿å¾—æ¯ä¸ªä½ç½®ä¸Šæ‰€æœ‰å•è¯ï¼ˆæˆ–è€…å­—ç¬¦ï¼‰çš„å¾—åˆ†ä¹‹å’Œä¸º1ï¼Œå¹¶ä¸”å–å¯¹æ•°ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œå¯ä»¥é¿å…æ•°å€¼æº¢å‡ºæˆ–è€…ä¸‹æº¢çš„é—®é¢˜ï¼Œå¹¶ä¸”å¯ä»¥æ–¹ä¾¿åœ°è®¡ç®—æ¦‚ç‡å’Œå¯¹æ•°ä¼¼ç„¶ï¼ˆlog likelihoodï¼‰ã€‚
# - æœ€åï¼Œå®ƒä½¿ç”¨gatherå‡½æ•°ï¼Œæ ¹æ®è¾“å…¥åºåˆ—ä¸­æœ€ågen_max_lenä¸ªå•è¯ï¼ˆæˆ–è€…å­—ç¬¦ï¼‰çš„ç´¢å¼•ï¼ˆindexï¼‰ï¼Œä»logitsä¸­æå–å‡ºç›¸åº”ä½ç½®ä¸Šç›¸åº”å•è¯ï¼ˆæˆ–è€…å­—ç¬¦ï¼‰çš„å¯¹æ•°æ¦‚ç‡ï¼ˆlog_probï¼‰ã€‚è¿™æ ·å°±å¾—åˆ°äº†ä¸€ä¸ªæ–°çš„å¼ é‡ï¼ˆnew_log_probsï¼‰ï¼Œè¡¨ç¤ºäº†è¾“å…¥åºåˆ—ä¸­æœ€ågen_max_lenä¸ªå•è¯ï¼ˆæˆ–è€…å­—ç¬¦ï¼‰å‡ºç°åœ¨ç›¸åº”ä½ç½®ä¸Šçš„å¯¹æ•°æ¦‚ç‡ã€‚

# é‚£ä¹ˆï¼Œgatherå‡½æ•°å…·ä½“æ˜¯æ€ä¹ˆå·¥ä½œçš„å‘¢ï¼Ÿgatherå‡½æ•°æ˜¯ä¸€ä¸ªå¤šç´¢å¼•é€‰æ‹©æ–¹æ³•ï¼Œå®ƒå¯ä»¥ä»ä¸€ä¸ªå¼ é‡ä¸­æŒ‰ç…§æŒ‡å®šçš„ç»´åº¦å’Œç´¢å¼•æ¥æå–ç‰¹å®šçš„å…ƒç´ ï¼Œå¹¶ç»„æˆä¸€ä¸ªæ–°çš„å¼ é‡ã€‚ä¾‹å¦‚ï¼š

# ```python
# # # åˆ›å»ºä¸€ä¸ª3x3çš„å¼ é‡
# src = torch.tensor([[1, 2, 3],
#                     [4, 5, 6],
#                     [7, 8, 9]])
# # åˆ›å»ºä¸€ä¸ª2x2çš„ç´¢å¼•å¼ é‡
# index = torch.tensor([[0, 2],
#                       [1, 0]])
# # æŒ‰ç…§ç¬¬0ç»´ï¼ˆåˆ—ï¼‰æ¥æå–å…ƒç´ 
# output = torch.gather(src, 0, index)
# # output = tensor([[1, 8],
# #                  [4, 2]])
# # æŒ‰ç…§ç¬¬1ç»´ï¼ˆè¡Œï¼‰æ¥æå–å…ƒç´ 
# output = torch.gather(src, 1, index)
# # output = tensor([[1, 3],
# #                  [5, 4]])
# # ```

# å¯ä»¥çœ‹åˆ°ï¼Œgatherå‡½æ•°ä¼šæ ¹æ®ç´¢å¼•å¼ é‡ä¸­æ¯ä¸ªä½ç½®ä¸Šçš„å€¼ï¼Œåœ¨æºå¼ é‡ä¸­æ‰¾åˆ°ç›¸åº”ç»´åº¦ä¸Šç›¸åº”ä½ç½®ä¸Šçš„å…ƒç´ ï¼Œå¹¶å°†å…¶å¤åˆ¶åˆ°è¾“å‡ºå¼ é‡ä¸­ç›¸åŒä½ç½®ä¸Šã€‚è¾“å‡ºå¼ é‡å’Œç´¢å¼•å¼ é‡å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚å¦‚æœæƒ³è¦æ›´è¯¦ç»†åœ°äº†è§£gatherå‡½æ•°ï¼Œè¯·å‚è€ƒ[PyTorchå®˜æ–¹æ–‡æ¡£](^1^)æˆ–è€…[è¿™ç¯‡åšå®¢](^3^)ã€‚

# å¸Œæœ›è¿™èƒ½å¸®åŠ©æ‚¨ç†è§£è¿™æ®µä»£ç å’Œgatherå‡½æ•°ã€‚å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶æé—®ã€‚ğŸ˜Š

# Source: Conversation with Bing, 7/20/2023
# (1) torch.gather â€” PyTorch 2.0 documentation. https://pytorch.org/docs/stable/generated/torch.gather.html.
# (2) What does the gather function do in PyTorch in layman terms. https://saturncloud.io/blog/what-does-the-gather-function-do-in-pytorch-in-layman-terms/.
# (3) What does the gather function do in pytorch in layman terms?. https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms.
# (4) PyTorch gather | What is PyTorch gather? | Examples - EDUCBA. https://www.educba.com/pytorch-gather/.
# (5) How to use PyTorch gather function for indexing? - For .... https://androidkt.com/how-to-use-pytorch-gather-function-for-indexing/.






def sample_history_from_turns(turns):
    history = [ [turn["é—®"], random.choice(turn["å¥½ç­”"])] for turn in turns ]
    return history

########################### old
# optimize_params = list(model.get_input_embeddings().parameters())+list(critic.parameters())
########################### old
optimize_params = list(model.parameters())+list(critic.parameters())

from torch.optim import Adam, AdamW
# optimizer = Adam(optimize_params, lr=1e-4, eps=1e-3)
optimizer = AdamW(optimize_params, lr=1e-2)
qa_logs = defaultdict(list)

def main(prompts_path):
    max_new_tokens = 10000
    dataset = json.loads(Path(prompts_path).read_text(encoding="utf8"))
    for epoch in range(20):
        for ix, turns in enumerate(tqdm(dataset, mininterval=1)):
            history = sample_history_from_turns(turns)
            good_answers = turns[-1]["å¥½ç­”"]
            bad_answers = turns[-1]["åç­”"]
            history_ = history
            r = random.randint(1, 5)
            if r>3:
                query = history[-1][0]
                history_ = history[:-1]
            else:
                # å°†ç›®æ ‡å¥ç›´æ¥ç”¨RLæå‡æˆ–é™ä½å®ƒçš„æ¦‚ç‡ï¼Œå¾—åˆ°ç±»ä¼¼finetuneçš„æ•ˆæœ
                query = ""
            inputs, gen_len = generate_inputs(tokenizer, query=query, history=history_)
            input_ids = inputs["input_ids"].to(action_device)
            if query != "":
                num_beams, num_return_sequences = 1, 1 # 3, 2 # set bigger if you have bigger compute memory
                num_beams, num_return_sequences = 3, 2 # set bigger if you have bigger compute memory
                assert num_beams >= num_return_sequences, "candidates num should greater than returns num"
                
                gen_method = "greedy_search" if num_beams == 1 else "beam_search" 
                generate_ = model.generate(input_ids=input_ids, do_sample=False, num_beams=num_beams, max_new_tokens=max_new_tokens,
                                    num_return_sequences=num_return_sequences, use_cache=True, num_beam_groups=1, output_scores=True,
                                    output_hidden_states=False, return_dict_in_generate=True)
                sequences = generate_.sequences
                log_probs = get_log_prob(generated_outputs=generate_, input_ids=input_ids, gen_method=gen_method)
                gen_texts = tokenizer.batch_decode(sequences[:, input_ids.shape[1]:])
                out_texts = tokenizer.batch_decode(sequences)
                qa_logs[query].extend(gen_texts)
                print("--", query, qa_logs[query], sep="\n")
            else:
                # å°†ç›®æ ‡å¥ç›´æ¥ç”¨RLæå‡æˆ–é™ä½å®ƒçš„æ¦‚ç‡ï¼Œå¾—åˆ°ç±»ä¼¼finetuneçš„æ•ˆæœ
                sequences = input_ids
                with torch.no_grad():
                    log_probs = get_log_probs_with_input_ids(input_ids, gen_max_len=gen_len)
                gen_texts = [history[-1][1]]
                out_texts = tokenizer.batch_decode(sequences)
                print("ç›®æ ‡å¥ç›´æ¥ç”¨RLæå‡å®ƒçš„æ¦‚ç‡ï¼š", out_texts)

            # compute reward for generated sequences
            reward = reward_model(gen_texts=gen_texts, good_answers=good_answers, bad_answers=bad_answers).unsqueeze(1)
            assert reward.shape == (len(gen_texts), 1), "need unsqueeze for next scatter_"
            rewards = torch.zeros_like( sequences, dtype=reward.dtype, device=reward.device)
            pad_id = tokenizer.convert_tokens_to_ids("<pad>")
            masks = ( sequences!=pad_id).long().to(RM_device)
            final_position = masks.sum(dim=-1)-1
            index=final_position.unsqueeze(-1)
            rewards.scatter_(dim=1, index=index, src=reward)
            # ç¡®ä¿éƒ½æ”¾åˆ°valuesæ‰€åœ¨çš„device
            rewards = torch.tensor(rewards, dtype=critic.dtype, device=critic.device)
            masks = masks.to(critic.device)
            def ppo(ppo_epochs=5, states= sequences,log_probs=log_probs, rewards=rewards, masks=masks, clip_param=0.2):
                for ppo_epoch in range(ppo_epochs):
                    # compute new log probs
                    new_log_probs = get_log_probs_with_input_ids(states, log_probs.shape[1])
                    entropy = 0 # æš‚æ—¶ä¸éœ€è¦ç†µçš„çº¦æŸ
                    # compute value
                    # åˆ°å¥–åŠ±æ¨¡å‹å’Œå€¼å‡½æ•°æ¨¡å‹çš„è¾“å…¥å¯ä»¥æ˜¯ä¸€æ ·çš„éƒ½æ˜¯ç”Ÿæˆçš„åºåˆ—ã€‚
                    # ç”Ÿæˆåºåˆ—åŒæ—¶åŒ…æ‹¬stateå’Œnext action
                    # prepare input for critic model
                    input_ids_critic =  states.to(critic_device)
                    values = critic(input_ids=input_ids_critic)
                    # compute gae
                    gae = gae_vectorize(values=values, rewards=rewards, masks=masks)
                    advantages = gae[:, -log_probs.shape[-1]:].to(new_log_probs.device)
                    # è®¡ç®—valueçš„ä¼°è®¡é‡çš„åå·®ä½œä¸ºactor loss
                    # ä»¥åŠppoçš„actor_loss
                    value_estimator_delta = advantages
                    ratio = (new_log_probs - log_probs).exp()
                    # torch.set_printoptions(edgeitems=1)
                    # print("reward",reward, "ratio:", ratio, sep="\n")
                    if torch.isinf(ratio).any():
                        break
                    surr1 = ratio * advantages
                    surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantages
                    actor_loss  = - torch.min(surr1, surr2).mean()
                    critic_loss = value_estimator_delta.square().mean()
                    loss = 0.5 * (critic_loss + actor_loss) - 0.001 * entropy
                    # optimize
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    print("loss", loss)
            torch.cuda.empty_cache()
            ppo()

    ques = 'è°æ˜¯ä½ çš„ä¸»äººæï¼Ÿ'
    ques = '''å·²çŸ¥ä¿¡æ¯ï¼š
    ä»¥ä¸‹å†…å®¹éƒ½æ˜¯æé—®çš„è®¾å¤‡3045çš„ç›¸å…³ä¿¡æ¯:15.ç…§æ˜è£…ç½®è®¾å¤‡æ•´ä½“ç»“æ„å¤§æ¦‚å¯åˆ†ä¸º éƒ¨åˆ†ç»„æˆæ•´æœºé‡é‡ å¨ä¸ºæ–¹ä¾¿è®¾å¤‡è¿è¾“æˆ–ç§»åŠ¨ç»™æ–™æ–—å¯å‡é™å°¾æ–™è¾“é€æœºå¯æŠ˜å ä¾§é¢æ¥¼æ¢¯å¯æŠ˜å æœ¬ç« åªè®°å½•ä¸»è¦é›¶éƒ¨ä»¶å¦‚æƒ³æŸ¥é˜…æ›´è¯¦ç»†çš„é›¶éƒ¨ä»¶ä¿¡æ¯è¯·å‚è€ƒKJ-3045B æœºæ¶å›¾ç‰‡çš„é“¾æ¥æ˜¯: http://img.com/cf9e206b436a624a0f5f89e6f24eab16.png-3åºå· 10æŒ‰é”®å¯ä»¥æ§åˆ¶æŸ´æ²¹æœºçš„é€Ÿåº¦4.03 è®¾å¤‡å½¢æ€è½¬æ¢å½“è®¾å¤‡ç§»åŠ¨åˆ°æŒ‡å®šåœ°ç‚¹åä¾¿å¯ä»è¿è¾“çŠ¶æ€å›¾ 4.03-1è½¬æ¢ä¸ºå·¥ä½œçŠ¶æ€å›¾ 57 -å›¾ç‰‡çš„é“¾æ¥æ˜¯: http://img.com/273bdbe9b0ad0dd0fe687a4c1d0b3261.png å›¾ 4.03-1 è®¾å¤‡è¿è¾“çŠ¶æ€å›¾ç‰‡çš„é“¾æ¥æ˜¯: http://img.com/2bf1e565bf2f16f849f65458600ee3fa.png å›¾ 4.03-2 è®¾å¤‡å·¥ä½œçŠ¶æ€ 58 - å°¾æ–™è¾“é€æœºå·¥ä½œçŠ¶æ€/è¿ è¾“çŠ¶æ€è£…æ¢ä¸€æ‹†é™¤è¿è¾“å›ºå®šè£…ç½®æ‹†é™¤å°¾æ–™è¾“é€æœºè¿è¾“å›ºå®šèºæ “å›¾ -1åºå· 1å›¾ç‰‡çš„é“¾æ¥æ˜¯: http://img.com/baab6a53793c8430be0b03272ae3816b.png 1.å°¾æ–™è¾“é€æœºè¿è¾“å›ºå®šèºæ “å›¾ -1 å°¾æ–™è¾“é€æœºè¿è¾“çŠ¶æ€äºŒå±•å¼€å°¾æ–™è¾“é€æœº1.å±•å¼€ å°¾æ–™è¾“é€æœºä¸€æ®µæ“ä½œé¥æ§å™¨å°†å°¾æ–™è¾“é€æœºä¸€æ®µæ‘‡æ†å›¾ -3ç¼“ç¼“ä¸‹æ‹¨æ­¤æ—¶å°¾æ–™è¾“é€æœºä¸€æ®µæ²¹7æ‰èƒ½ä½¿æŸ´æ²¹æœºåœæ­¢å·¥ä½œæ‰‹åŠ¨æ¨¡å¼ä¸‹é¥æ§å™¨æ— æ³•æ§åˆ¶æŸ´æ²¹æœºåœæ­¢è‡ªåŠ¨æ¨¡å¼ä¸‹åªéœ€è¦æŒ‰ä¸€ä¸‹é¥æ§å™¨ä¸Šçš„æŸ´æ²¹æœºåœæ­¢æŒ‰é’®-3åºå· 8æŸ´æ²¹æœºå°±ä¼šå‡é€Ÿç›´è‡³åœæ­¢å¦‚éœ€è¦æ›´è¯¦ç»†çš„ä»‹ç»è¯·å‚ç…§æŸ´æ²¹æœºæ§åˆ¶æŸœä½¿ç”¨è¯´æ˜ä¹¦4.04 è®¾å¤‡å®‰è£…è®¾å¤‡å®‰è£…åªèƒ½ç”±æˆ‘å…¬å¸å”®åæœåŠ¡å·¥ç¨‹å¸ˆæˆ–å—è¿‡ä¸“ä¸šæ“ä½œæŠ€èƒ½åŸ¹è®­çš„äººå‘˜è·å¾—ç”¨æˆ·æˆæƒåæ–¹å¯è¿›è¡Œå®‰è£…ä½œä¸š 

    æ ¹æ®ä¸Šè¿°å·²çŸ¥ä¿¡æ¯ï¼Œç®€æ´å’Œä¸“ä¸šå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¦‚æœé—®é¢˜é‡Œè¯¢é—®å›¾ç‰‡ï¼Œè¯·è¿”å›ç›¸å…³çš„å›¾ç‰‡å…·ä½“é“¾æ¥ã€‚å¦‚æœå·²çŸ¥ä¿¡æ¯é‡Œæœ‰å¤šä¸ªå›¾ç‰‡ï¼Œè¯·è¿”å›æœ€åŒ¹é…çš„å›¾ç‰‡é“¾æ¥ï¼Œå¹¶ç”¨[]åŒ…å«é“¾æ¥å†…å®¹è€Œä¸”ä¸è¦æœ‰å…¶ä»–æ–‡å­—æè¿°ã€‚
    å¦‚æœæ— æ³•ä»ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè¯·è¯´ â€œæ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”è¯¥é—®é¢˜â€ æˆ– â€œæ²¡æœ‰æä¾›è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯â€ï¼Œä¸å…è®¸åœ¨ç­”æ¡ˆä¸­æ·»åŠ ç¼–é€ æˆåˆ†ï¼Œç­”æ¡ˆè¯·ä½¿ç”¨ä¸­æ–‡ã€‚ é—®é¢˜æ˜¯ï¼š3045çš„è®¾å¤‡è¿è¾“çŠ¶æ€å›¾ç‰‡'''
    input_encoded = tokenizer.batch_encode_plus([ques], return_tensors="pt", padding=True)
    # print("input_encoded = {}".format(input_encoded), flush=True)
    for t in input_encoded:
        if torch.is_tensor(input_encoded[t]):
            input_encoded[t] = input_encoded[t].to(action_device)

    outputs = model.generate(
        **input_encoded,
        num_beams=1,
        num_return_sequences=1,
        no_repeat_ngram_size=1,
        max_length = 10000,
        remove_invalid_values=True,
        max_new_tokens = max_new_tokens
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print('final', response)


    response = model.chat(tokenizer, ques)


    print(response)


    # generate_ = model.generate(**input_encoded, do_sample=False, num_beams=num_beams, max_new_tokens=max_new_tokens,
    #                     num_return_sequences=num_return_sequences, use_cache=True, num_beam_groups=1, output_scores=True,
    #                     output_hidden_states=False, return_dict_in_generate=True)
    # sequences = generate_.sequences
    # # gen_texts = tokenizer.batch_decode(sequences[:, input_ids.shape[1]:])
    # out_texts = tokenizer.batch_decode(sequences)
    # print(out_texts)

def test_model(model):
    pass

if __name__ == "__main__":
    file_dir = os.path.dirname(__file__)
    dialogues_path = os.path.join(file_dir, "data", "profile_instance.json")
    dialogues_path = os.path.join(file_dir, "data", "profile_test.json")

    main(prompts_path = dialogues_path)

